ipython /Users/johnbeieler/Dropbox/Data\ Mining/Final\ Paper/code.py
===============

Running for data: Prediction

===============


Running RF



Random Forest best estimator: RandomForestClassifier(bootstrap=True, compute_importances=None,
            criterion=gini, max_depth=None, max_features=auto,
            min_density=None, min_samples_leaf=1, min_samples_split=1,
            n_estimators=250, n_jobs=1, oob_score=False, random_state=0,
            verbose=0)

Random Forest gridsearchCV scores: [mean: 0.91263, std: 0.00455, params: {'n_estimators': 100}, mean: 0.91294, std: 0.00310, params: {'n_estimators': 250}, mean: 0.91263, std: 0.00126, params: {'n_estimators': 500}, mean: 0.91202, std: 0.00118, params: {'n_estimators': 750}, mean: 0.91263, std: 0.00075, params: {'n_estimators': 1000}]

Random forest precision: 0.80829015544

Random forest recall: 0.739336492891

Random forest f1_score: 0.772277227723

AUC is 0.957702637708
Running GBT


AdaBoost best estimator: AdaBoostClassifier(algorithm=SAMME.R,
          base_estimator=DecisionTreeClassifier(compute_importances=None, criterion=gini,
            max_depth=None, max_features=None, min_density=None,
            min_samples_leaf=1, min_samples_split=2, random_state=None,
            splitter=best),
          base_estimator__compute_importances=None,
          base_estimator__criterion=gini, base_estimator__max_depth=None,
          base_estimator__max_features=None,
          base_estimator__min_density=None,
          base_estimator__min_samples_leaf=1,
          base_estimator__min_samples_split=2,
          base_estimator__random_state=None, base_estimator__splitter=best,
          learning_rate=1.0, n_estimators=100, random_state=None)

AdaBoost gridsearchCV scores: [mean: 0.88841, std: 0.01469, params: {'n_estimators': 100, 'algorithm': 'SAMME'}, mean: 0.88657, std: 0.00922, params: {'n_estimators': 250, 'algorithm': 'SAMME'}, mean: 0.88964, std: 0.01236, params: {'n_estimators': 500, 'algorithm': 'SAMME'}, mean: 0.89056, std: 0.01365, params: {'n_estimators': 750, 'algorithm': 'SAMME'}, mean: 0.89086, std: 0.01017, params: {'n_estimators': 1000, 'algorithm': 'SAMME'}, mean: 0.89209, std: 0.01246, params: {'n_estimators': 100, 'algorithm': 'SAMME.R'}, mean: 0.89025, std: 0.01571, params: {'n_estimators': 250, 'algorithm': 'SAMME.R'}, mean: 0.88780, std: 0.01038, params: {'n_estimators': 500, 'algorithm': 'SAMME.R'}, mean: 0.88749, std: 0.01081, params: {'n_estimators': 750, 'algorithm': 'SAMME.R'}, mean: 0.89148, std: 0.00848, params: {'n_estimators': 1000, 'algorithm': 'SAMME.R'}]

AdaBoost precision: 0.67264573991

AdaBoost recall: 0.710900473934

AdaBoost f1_score: 0.691244239631

AUC is 0.813831080752
Running Logit



Logistic Regression best estimator: LogisticRegression(C=1, class_weight=auto, dual=False, fit_intercept=True,
          intercept_scaling=1, penalty=l1, random_state=None, tol=0.0001)

Logistic Regression gridsearchCV scores: [mean: 0.89117, std: 0.01311, params: {'penalty': 'l1', 'C': 0.0625, 'class_weight': 'auto'}, mean: 0.89086, std: 0.01023, params: {'penalty': 'l2', 'C': 0.0625, 'class_weight': 'auto'}, mean: 0.87676, std: 0.00658, params: {'penalty': 'l1', 'C': 0.0625, 'class_weight': None}, mean: 0.88688, std: 0.00599, params: {'penalty': 'l2', 'C': 0.0625, 'class_weight': None}, mean: 0.89393, std: 0.01325, params: {'penalty': 'l1', 'C': 0.125, 'class_weight': 'auto'}, mean: 0.89270, std: 0.00977, params: {'penalty': 'l2', 'C': 0.125, 'class_weight': 'auto'}, mean: 0.88473, std: 0.00515, params: {'penalty': 'l1', 'C': 0.125, 'class_weight': None}, mean: 0.88811, std: 0.00629, params: {'penalty': 'l2', 'C': 0.125, 'class_weight': None}, mean: 0.89393, std: 0.00931, params: {'penalty': 'l1', 'C': 0.25, 'class_weight': 'auto'}, mean: 0.89393, std: 0.01090, params: {'penalty': 'l2', 'C': 0.25, 'class_weight': 'auto'}, mean: 0.88841, std: 0.00607, params: {'penalty': 'l1', 'C': 0.25, 'class_weight': None}, mean: 0.89025, std: 0.00571, params: {'penalty': 'l2', 'C': 0.25, 'class_weight': None}, mean: 0.89362, std: 0.00834, params: {'penalty': 'l1', 'C': 0.5, 'class_weight': 'auto'}, mean: 0.89148, std: 0.01011, params: {'penalty': 'l2', 'C': 0.5, 'class_weight': 'auto'}, mean: 0.88903, std: 0.00629, params: {'penalty': 'l1', 'C': 0.5, 'class_weight': None}, mean: 0.89025, std: 0.00462, params: {'penalty': 'l2', 'C': 0.5, 'class_weight': None}, mean: 0.89424, std: 0.00638, params: {'penalty': 'l1', 'C': 1, 'class_weight': 'auto'}, mean: 0.89240, std: 0.01122, params: {'penalty': 'l2', 'C': 1, 'class_weight': 'auto'}, mean: 0.88933, std: 0.00462, params: {'penalty': 'l1', 'C': 1, 'class_weight': None}, mean: 0.89301, std: 0.00587, params: {'penalty': 'l2', 'C': 1, 'class_weight': None}, mean: 0.89209, std: 0.00374, params: {'penalty': 'l1', 'C': 2, 'class_weight': 'auto'}, mean: 0.89148, std: 0.00787, params: {'penalty': 'l2', 'C': 2, 'class_weight': 'auto'}, mean: 0.89117, std: 0.00343, params: {'penalty': 'l1', 'C': 2, 'class_weight': None}, mean: 0.89240, std: 0.00348, params: {'penalty': 'l2', 'C': 2, 'class_weight': None}, mean: 0.89056, std: 0.00591, params: {'penalty': 'l1', 'C': 4, 'class_weight': 'auto'}, mean: 0.89332, std: 0.00822, params: {'penalty': 'l2', 'C': 4, 'class_weight': 'auto'}, mean: 0.89086, std: 0.00343, params: {'penalty': 'l1', 'C': 4, 'class_weight': None}, mean: 0.89301, std: 0.00193, params: {'penalty': 'l2', 'C': 4, 'class_weight': None}, mean: 0.89117, std: 0.00634, params: {'penalty': 'l1', 'C': 8, 'class_weight': 'auto'}, mean: 0.89117, std: 0.00651, params: {'penalty': 'l2', 'C': 8, 'class_weight': 'auto'}, mean: 0.89086, std: 0.00343, params: {'penalty': 'l1', 'C': 8, 'class_weight': None}, mean: 0.89117, std: 0.00382, params: {'penalty': 'l2', 'C': 8, 'class_weight': None}, mean: 0.88994, std: 0.00607, params: {'penalty': 'l1', 'C': 16, 'class_weight': 'auto'}, mean: 0.89056, std: 0.00663, params: {'penalty': 'l2', 'C': 16, 'class_weight': 'auto'}, mean: 0.88994, std: 0.00390, params: {'penalty': 'l1', 'C': 16, 'class_weight': None}, mean: 0.89086, std: 0.00268, params: {'penalty': 'l2', 'C': 16, 'class_weight': None}]

Logistic reg precision: 0.633333333333

Logistic reg recall: 0.810426540284

Logistic reg f1_score: 0.711018711019

AUC is 0.906067107275