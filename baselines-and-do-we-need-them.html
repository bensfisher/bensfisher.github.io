<!DOCTYPE html>
<html lang="en">

  <head>
      <title>Ben Fisher</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="./theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="./theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href=".">Ben Fisher</a></h1>
        <h3 class ="sitesubtitle"></h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
            <!--pages-->
                <li><a class="nodec" href="./pages/about.html">About Me</a></li>
            <!-- services icons -->
              <li><a class="nodec icon-mail-alt" href="mailto:bensfisher@gmail.com"></a></li>
              <li><a class="nodec icon-github" href="https://github.com/bensfisher"></a></li>
              <li><a class="nodec icon-twitter" href="https://twitter.com/matachin_tower"></a></li>
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->

<section class="content">

  <h3 class="posttitle">
    <a class="nodec" href="/baselines-and-do-we-need-them.html" rel="bookmark" title="Permalink to Baselines and do we need them?">
      Baselines and do we need them?
    </a>
  </h3>

  <div class="postinfo">
    <p class="published" title="2014-09-23T00:00:00">
      Tue 23 September 2014
    </p>

  </div><!-- .postinfo -->

  <div class="article">
    <p>As someone whose research focuses on developing statistical forecasting models,
one of the big questions that frequently comes up is how good the model is
compared to simply guessing. I generally think of this concept of a pure chance
model as the baseline. While it seems like a fairly straightforward comparison,
it becomes much more complicated when dealing with imbalanced data.</p>
<p>The natural example of a baseline is the coin flip. Is a given predictive model
with a binary dependent variable more accurate than simply flipping a coin? 
It's an important comparison, but only for data that has an even split between positive
and negative cases. I have yet to work with international relations data where 
the dependent variable has a fifty-fifty split.</p>
<p>There are a couple of options for unbalanced baselines that I've seen brought up.
The first is whether a model's accuracy is an improvement over naively predicting 
every observation to be a 0. I'm not convinced that this is a useful comparison,
because there's always a certain amount of leeway that we give to false positives. 
This may leave us with worse mean accuracy than guessing all 0's, but it doesn't 
matter if we think the correct predictions are worth a few false postives. </p>
<p>The other option, which sparked this blog post, is to randomly classify a portion 
of the obesrvations as 1's at the rate at which positive observations occur in 
the data. As an example,if positive observations make up 2.5% of a dataset, then 
you would randomly predict 2.5% of the observations as positives and compare
the precision to that of the model's predicted positives. I thought this was a
fairly intuitive way to approach the problem when I first heard it. However, 
I became more skeptical the more I thought about it. This approach assumes that 
false positives are random, which is never the case in my experience with political 
forecasting.</p>
<p>Going back to the mass atrocities example, I wouldn't consider Iraq showing up 
as a false positive to be a big issue. After all, this is a country that has a 
history of severe instability,so it showing up as a false positive wouldn't be 
a big surprise. Someplace like Belgium or Denmark showing up as high risk would 
be a different story. Basically, the characteristics of your false positives matter.
The rare, semi-random nature of events like mass atrocities means that false positives
are always going to be an issue, but the real test of a model comes from what
those false positives are.</p>
<p>I think the larger point to take away from all this is that you have to do more
than just throw out accuracy metrics when assessing a model. As I've pointed out,
baseline metrics are problematic in a lot of cases for social science data. You need to 
actually look at the individual results. You'll get a better understanding of
the kind of predictions your model is generating and hopefully why the model 
is accurate in some cases and not others. </p>
  </div><!-- .content -->

</section>


    <!-- footer -->
    <footer>
      <p>
        Â© Ben Fisher, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>